# ü¶æ TTF-VLA: Temporal Token Fusion for Vision-Language-Action Models

[![Paper](https://img.shields.io/badge/Paper-arXiv%3A2508.19257-b31b1b.svg)](https://arxiv.org/abs/2508.19257)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](./LICENSE)

A training-free inference optimization method for enhancing VLA model performance through intelligent temporal visual information integration.  

Project and repository led and maintained by [Chenghao Liu](https://github.com/MrCapricornLiu).

We are thrilled to announce that our paper has been accepted by **AAAI 2026** üéâ. Read it on [arXiv](https://arxiv.org/abs/2508.19257).

## üì¶ Repository Structure

```bash
TTF-VLA/
‚îú‚îÄ‚îÄ experiments/robot/
‚îÇ   ‚îú‚îÄ‚îÄ aptube_manager.py           # Core TTF implementation
‚îÇ   ‚îú‚îÄ‚îÄ libero/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ run_libero_eval_aptube.py  # Core evaluation script (LIBERO Env)
‚îÇ   ‚îî‚îÄ‚îÄ openvla_utils.py            # VLA model utilities
‚îú‚îÄ‚îÄ prismatic/extern/hf/
‚îÇ   ‚îî‚îÄ‚îÄ modeling_prismatic.py       # Model integration points
‚îî‚îÄ‚îÄ README.md                       # This file
```

> Note: **aptube** is the old name of TTF. To avoid unnecessary bugs, we kept it unchanged in the code.

## ‚öôÔ∏è Setup

If you don't have mamba, run the following command first:

```bash
conda install -n base -c conda-forge mamba
mamba --version
mamba shell init --shell bash
exec $SHELL -l
```

Either mamba or conda works; mamba is much faster.
**If you prefer conda**, simply replace all `mamba` commands below with `conda`.

```bash
mamba create -n ttfvla python=3.10 -y
mamba activate ttfvla
mamba install pytorch torchvision torchaudio pytorch-cuda=12.4 -c pytorch -c nvidia -y  

# pwd: ~/TTF-VLA
# VLA-related packages
pip install -e .

# LIBERO Environment packages
git clone https://github.com/Lifelong-Robot-Learning/LIBERO.git
cd LIBERO 
# pwd: ~/TTF-VLA/LIBERO
pip install -e .

cd .. 
# pwd: ~/TTF-VLA
pip install -r experiments/robot/libero/libero_requirements.txt
```

‚ö†Ô∏è Afterward, this error may occur:
```
ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow 2.15.0 requires numpy<2.0.0,>=1.23.5, but you have numpy 2.2.6 which is incompatible.
```
Then just run 
```bash
pip install numpy==1.26
```
‚ö†Ô∏è After that, **it will report a conflict error between numpy and tensorflow, just ignore it.**

Because we need to extract attention during inference, we don't use flash attention. Don‚Äôt worry, this has almost no impact on inference speed for VLA models.

## üß© Download Pretrained Models

```bash
mamba activate ttfvla
python download-ckpt-scripts/download_model_local.py  --model_id openvla/openvla-7b-finetuned-libero-spatial
```
You can change the model_id to download other models as needed:
- `openvla/openvla-7b-finetuned-libero-object`
- `openvla/openvla-7b-finetuned-libero-goal`
- `openvla/openvla-7b-finetuned-libero-10`

Note that openvla-7b-finetuned-libero-10 is the name of model finetuned on LIBERO-Long task suite.

## üß™ Evaluation on LIBERO Tasks

```bash
# Evaluate OpenVLA + TTF on Object task suite
# pwd: ~/TTF-VLA
python experiments/robot/libero/run_libero_eval_aptube.py \
    --pretrained_checkpoint "checkpoints/openvla-7b-finetuned-libero-object" \
    --task_suite_name "libero_object" \
    --center_crop True \
    --aptube_enabled True \
    --fusion_mode "attention_guided" \
    --keyframe_interval 3 \
    --patch_diff_threshold 0.03 \
    --attention_top_k 70 \
    --num_trials_per_task 20
```

**Available task suites:**

* üß± `libero_object` ‚Äî Object manipulation tasks
* üìê `libero_spatial` ‚Äî Spatial reasoning tasks
* üéØ `libero_goal` ‚Äî Goal-conditioned tasks
* ‚è±Ô∏è `libero_10` ‚Äî Long-horizon tasks

## üìñ Citation

If you find this work useful, please cite:

```bibtex
@article{liu2025ttf,
  title={TTF-VLA: Temporal Token Fusion via Pixel-Attention Integration for Vision-Language-Action Models},
  author={Liu, Chenghao and Zhang, Jiachen and Li, Chengxuan and Zhou, Zhimu and Wu, Shixin and Huang, Songfang and Duan, Huiling},
  journal={arXiv preprint arXiv:2508.19257},
  doi={10.48550/arXiv.2508.19257},
  url={https://arxiv.org/abs/2508.19257}
}
```

## üôè Acknowledgements

We build upon the excellent works of [OpenVLA](https://github.com/openvla/openvla) and [VLA-Cache](https://github.com/siyuhsu/vla-cache). We sincerely appreciate their great work.

